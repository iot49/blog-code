#!/usr/bin/env python3
"""
bin/check-links — Check external links in the built site.

Usage:
  ./bin/check-links              # build first, then check
  ./bin/check-links --no-build   # skip build, use existing dist/
  ./bin/check-links --timeout 10 # custom timeout in seconds (default: 8)
  ./bin/check-links --workers 20 # parallel workers (default: 10)
"""

import argparse
import re
import subprocess
import sys
import urllib.error
import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Optional

# ── Config ────────────────────────────────────────────────────────────────────

REPO_ROOT = Path(__file__).parent.parent
DIST = REPO_ROOT / "dist"

# Domains known to block automated requests (false positives)
SKIP_DOMAINS = {
    "linkedin.com",
    "www.linkedin.com",
    "twitter.com",
    "x.com",
    "www.twitter.com",
    "facebook.com",
    "www.facebook.com",
    "instagram.com",
    "www.instagram.com",
    "web.archive.org",  # slow / rate-limited
    "dash.cloudflare.com",  # blocks automated checks (403)
}

# ── Helpers ───────────────────────────────────────────────────────────────────

RESET = "\033[0m"
RED = "\033[0;31m"
GREEN = "\033[0;32m"
YELLOW = "\033[1;33m"
CYAN = "\033[0;36m"
BOLD = "\033[1m"
DIM = "\033[2m"


def info(msg):
    print(f"{CYAN}▶{RESET} {msg}")


def ok(msg):
    print(f"{GREEN}✓{RESET} {msg}")


def warn(msg):
    print(f"{YELLOW}⚠{RESET}  {msg}")


def fail(msg):
    print(f"{RED}✗{RESET} {msg}")


def collect_external_links() -> dict[str, set[str]]:
    """Return {url: set_of_source_html_files} for all external links in dist/."""
    url_sources: dict[str, set[str]] = {}
    pattern = re.compile(r'href="(https?://[^"#?][^"]*)"')

    for html_file in DIST.rglob("*.html"):
        content = html_file.read_text(errors="ignore")
        for url in pattern.findall(content):
            # Normalise: strip trailing slash for dedup
            url = url.rstrip("/")
            url_sources.setdefault(url, set()).add(html_file.name)

    return url_sources


def should_skip(url: str) -> Optional[str]:
    """Return skip reason string if URL should be skipped, else None."""
    try:
        from urllib.parse import urlparse

        domain = urlparse(url).netloc
        if domain in SKIP_DOMAINS:
            return f"skipped (known false-positive domain: {domain})"
    except Exception:
        pass
    return None


def check_url(url: str, timeout: int) -> tuple:
    """
    Returns (url, status_code_or_None, error_or_None).
    Tries HEAD first, falls back to GET if HEAD returns 405.
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (compatible; link-checker/1.0; "
            "+https://github.com/onwidget/astrowind)"
        )
    }
    for method in ("HEAD", "GET"):
        try:
            req = urllib.request.Request(url, headers=headers, method=method)
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                return url, resp.status, None
        except urllib.error.HTTPError as e:
            if method == "HEAD" and e.code == 405:
                continue  # retry with GET
            return url, e.code, None
        except urllib.error.URLError as e:
            return url, None, str(e.reason)
        except Exception as e:
            return url, None, str(e)
    return url, None, "HEAD and GET both failed"


# ── Main ──────────────────────────────────────────────────────────────────────


def main():
    parser = argparse.ArgumentParser(description="Check external links in built site.")
    parser.add_argument("--no-build", action="store_true", help="Skip build step")
    parser.add_argument("--timeout", type=int, default=8, help="Request timeout (s)")
    parser.add_argument("--workers", type=int, default=10, help="Parallel workers")
    args = parser.parse_args()

    print(f"\n{BOLD}=== External link checker ==={RESET}\n")

    # ── Build ──────────────────────────────────────────────────────────────────
    if not args.no_build:
        info("Building site...")
        result = subprocess.run(
            ["npm", "run", "build", "--silent"],
            cwd=REPO_ROOT,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            fail("Build failed — fix build errors first.")
            print(result.stderr)
            sys.exit(1)
        ok("Build complete")
    else:
        if not DIST.exists():
            fail(f"dist/ not found at {DIST}. Run without --no-build first.")
            sys.exit(1)
        info("Using existing dist/ (--no-build)")

    # ── Collect links ──────────────────────────────────────────────────────────
    info("Collecting external links from built HTML...")
    url_sources = collect_external_links()

    if not url_sources:
        ok("No external links found.")
        return

    total = len(url_sources)
    print(f"  Found {total} unique external URL(s)\n")

    # ── Check links ────────────────────────────────────────────────────────────
    info(
        f"Checking links ({args.workers} parallel workers, {args.timeout}s timeout)...\n"
    )

    broken: list[tuple[str, str, set[str]]] = []  # (url, reason, sources)
    skipped: list[str] = []
    ok_count = 0

    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = {}
        for url in url_sources:
            reason = should_skip(url)
            if reason:
                skipped.append(url)
                print(f"  {DIM}~ {url}{RESET}")
                continue
            futures[executor.submit(check_url, url, args.timeout)] = url

        for future in as_completed(futures):
            url, status, error = future.result()
            sources = url_sources[url]

            if error:
                reason = f"Error: {error}"
                broken.append((url, reason, sources))
                print(f"  {RED}✗ {status or '???'}{RESET}  {url}")
                print(f"       {DIM}→ {reason}{RESET}")
            elif status and status >= 400:
                reason = f"HTTP {status}"
                broken.append((url, reason, sources))
                print(f"  {RED}✗ {status}{RESET}  {url}")
                print(f"       {DIM}found in: {', '.join(sorted(sources))}{RESET}")
            else:
                ok_count += 1
                print(f"  {GREEN}✓ {status}{RESET}  {url}")

    # ── Summary ────────────────────────────────────────────────────────────────
    print(f"\n{BOLD}=== Summary ==={RESET}")
    print(f"  {GREEN}✓ OK:{RESET}      {ok_count}")
    print(f"  {DIM}~ Skipped:{RESET} {len(skipped)}")
    print(f"  {RED}✗ Broken:{RESET}  {len(broken)}")

    if broken:
        print(f"\n{RED}{BOLD}Broken links:{RESET}")
        for url, reason, sources in sorted(broken):
            print(f"  {RED}✗{RESET} {url}")
            print(f"    Reason:  {reason}")
            print(f"    In file: {', '.join(sorted(sources))}")
        print()
        sys.exit(1)
    else:
        print(f"\n{GREEN}{BOLD}✓ All external links OK!{RESET}\n")


if __name__ == "__main__":
    main()
